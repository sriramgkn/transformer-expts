We envision this repo as a collection of our experiments with transformer models. 

Our first such experiment is with [DistilBERT](https://huggingface.co/docs/transformers/en/model_doc/distilbert), an efficient version of [BERT](https://huggingface.co/docs/transformers/en/model_doc/bert). We could easily run inference for [masked language modelling](https://huggingface.co/docs/transformers/main/en/tasks/masked_language_modeling) (MLM). We wanted to go further and full-finetune it on the [IMDB movie review dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) for text classification, but ran out of memory after a few minutes on Colab.
